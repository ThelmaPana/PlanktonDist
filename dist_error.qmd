---
title: "Error in distances from 3D to 2D"
subtitle: "Investigate whether 2D distances are representative of 3D distances using null hypothesis data."
format: 
  html:
    toc: true
    embed-resources: true
editor: visual
lightbox: true
cache: true
---

```{r set_up}
#| echo: false
#| output: false
#| cache: false
source("utils.R")

```

```{r read_data}
#| cache.lazy: false
# Read data
images <- read_parquet("data/00.images_clean.parquet")
plankton <- read_parquet("data/00.plankton_clean.parquet")

# Read null data
load("data/01.null_data.Rdata")
```

## Start with one example image

We draw a set of points within the volume of an ISIIS image. This volume is:

-   52.5 cm width = 10240 px (`x` dimension). Actually, the physical length of the image depends on the towing speed.

-   10.5 cm height = 2048 px (`y` dimension).

-   50 cm depth ≈ 9752 px (`z` dimension, compressed in images).

```{r}
# image volume
vol <- c()
vol$x <- 10240
vol$y <- 2048
vol$z <- 9572
```

Null data has been generated:

-   1000 images

-   number of objects within each image is representative of ISIIS data

-   within each image, objects are randomly distributed in 3D

```{r}
# Pick one image
r_points <- rand_points %>% 
  filter(img_name == "img_0001")

# Plot one image
r_points %>% 
  ggplot() +
  geom_point(aes(x = x, y = y, colour = z)) +
  scale_colour_viridis_c() +
  coord_fixed()
```

### Distance to 1st nearest neighbour (NN)

We can compute distances to nearest neighbour both in 2D and 3D and have a look at the correlation between. Let’s start with `spearman` correlation. First, we need to convert our dataframe into point pattern objects.

```{r}
#type_corr <- "spearman"
type_corr <- "pearson"

x2D <- ppp(r_points$x, r_points$y, window = owin(xrange = c(1, vol$x), yrange = c(1, vol$y)))
plot(x2D)
nnd2D <- nndist(x2D, k = 1)

x3D <- pp3(r_points$x, r_points$y, r_points$z, box3(xrange = c(1, vol$x), yrange = c(1, vol$y), zrange = c(1, vol$z)))
plot(x3D)
nnd3D <- nndist(x3D, k = 1)

tibble(nnd2D, nnd3D) %>% 
  ggplot() +
  geom_point(aes(x = nnd3D, y = nnd2D)) +
  geom_abline(slope = 1, color = "red") + 
  expand_limits(x = 0, y = 0) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  coord_fixed()

# We use spearman because of the number of points
cor(nnd2D, nnd3D, method = type_corr)
```

As expected, nnd2D are smaller or equal to nnd3D. When considering only one NN, the correlation is pretty bad. Let’s see what happens when we consider `k` neighbours.

### k neighbours

The number of NN to consider cannot exceed the `number of points - 1`.

```{r}
corr <- sapply(c(1:(nrow(r_points)-1)), function(k_nn) {
    nn2D <- nndist(x2D, k = k_nn)
    nn3D <- nndist(x3D, k = k_nn)
    cor(nn2D, nn3D, method = type_corr) 
})

res <- tibble(
  n_pt = nrow(r_points), # number of points in the set
  k_nn = c(1:(nrow(r_points)-1)), # number of considered NN
  prop_nn = k_nn/n_pt, # proportion of considered NN
  corr = corr # correlation
  )  

# Plot the result
res %>% 
  ggplot() +
  geom_path(aes(x = k_nn, y = corr))
```

We can also use the proportion of considered NN as x axis, to generalise when we vary the number of points within an image.

```{r}
res %>% 
  ggplot() +
  geom_path(aes(x = prop_nn, y = corr))
```

Now let’s check what happens when we vary the number of points within the image.

### Varying the number of points

```{r}
# Sizes of set of points
n_pts <- c(15:25)

# List of k NN to consider within each set
k_nn <- sapply(n_pts, function(i){
  seq(from = 2, to = i-1)
})

# List of size of set of points for df
n_pt <- sapply(n_pts, function(i){
  rep(i, length.out = i - 2)
})

# Df skeleton to store results
res <- tibble(
  k_nn = unlist(k_nn),
  n_pt = unlist(n_pt)
) %>% 
  mutate(prop_nn = k_nn / n_pt)

# Select images that have between 15 and 25 points (one image for each value)
img_sel <- rand_points %>% 
  count(img_name) %>% 
  filter(n %in% n_pts) %>% 
  group_by(n) %>% 
  slice_head(n = 1) %>% 
  ungroup()

corr <- sapply(n_pts, function(i){
 
  # Get image name
  img_name_i <- img_sel %>% filter(n == i) %>% pull(img_name)
  
  # Get points within this image
  r_points <- rand_points %>% filter(img_name == img_name_i)
  
  # Convert to point pattern
  x2D <- ppp(r_points$x, r_points$y, window = owin(xrange = c(1, 10240), yrange = c(1, 2048)))
  x3D <- pp3(r_points$x, r_points$y, r_points$z, box3(xrange = c(1, 10240), yrange = c(1, 2048), zrange = c(1, 10240)))
  
  # Compute distance to k NN and correlation between 2D and 3D distance
  sapply(c(2:(i-1)), function(k_nn) {
    nn2D <- nndist(x2D, k = k_nn)
    nn3D <- nndist(x3D, k = k_nn)
    cor(nn2D, nn3D, method = type_corr) 
  })
  
})

# Store result in dataframe
res <- res %>% 
  mutate(corr = unlist(corr))

# Plot
res %>% 
  ggplot() +
  geom_path(aes(x = prop_nn, y = corr)) +
  facet_wrap(~n_pt)
```

The strength of the correlation is sensible to the number of considered NN. Can we find an optimal proportion of NN to consider?

To answer this question, we need to draw numerous set of points of different sizes.

## Multiple images

### Fixed number of points and 1 NN

```{r}
# Nuber of points in each images
n_pt <- 20

# Get points within images with a fixed number of points
r_points <- rand_points %>% 
  add_count(img_name) %>% 
  filter(n == n_pt) %>% 
  select(-n)

# Compute correlations
corrs <- sapply(unique(r_points$img_name), function(i_img){# across images
  # Convert to point pattern
  r_points_img <- r_points %>% filter(img_name == i_img)
  x2D <- ppp(r_points_img$x, r_points_img$y, window = owin(xrange = c(1, 10240), yrange = c(1, 2048)))
  x3D <- pp3(r_points_img$x, r_points_img$y, r_points_img$z, box3(xrange = c(1, 10240), yrange = c(1, 2048), zrange = c(1, 10240)))
  
  # Compute NND with one neighbour
  nnd2D <- nndist(x2D, k = 1)
  nnd3D <- nndist(x3D, k = 1)
  
  # Compute correlation
  cor(nnd2D, nnd3D, method = type_corr)
})

# each value is the correlation for one image
summary(corrs)
```

### Fixed number of points and multiple NN

Now let’s vary the number of considered NN.

```{r}
#| warning: false
# Nuber of points in each images
n_pt <- 20

# Get points within images with a fixed number of points
r_points <- rand_points %>% 
  add_count(img_name) %>% 
  filter(n == n_pt) %>% 
  select(-n)


# Compute correlations
corrs <- sapply(unique(r_points$img_name), function(i_img){# across images
  # Convert to point pattern
  r_points_img <- r_points %>% filter(img_name == i_img)
  x2D <- ppp(r_points_img$x, r_points_img$y, window = owin(xrange = c(1, 10240), yrange = c(1, 2048)))
  x3D <- pp3(r_points_img$x, r_points_img$y, r_points_img$z, box3(xrange = c(1, 10240), yrange = c(1, 2048), zrange = c(1, 10240)))
  
  # Compute distance to k NN and correlation between 2D and 3D distance
  sapply(c(2:(n_pt-1)), function(k_nn) {
    nn2D <- nndist(x2D, k = k_nn)
    nn3D <- nndist(x3D, k = k_nn)
    cor(nn2D, nn3D, method = type_corr) 
  })
  
})



dim(corrs)
# rows represent the number of NN considered
# cols are images

# Convert to df
corrs <- as_tibble(corrs)
# Add number of considered NN
corrs <- corrs %>% 
  mutate(
    k_nn = 2:(n_pt - 1), 
    prop_nn = k_nn / n_pt
    ) %>% 
  select(k_nn, prop_nn, everything())
# Rearrange to vertical
corrs <- corrs %>% 
  pivot_longer(contains("img"), names_to = "img", values_to = "corr")

# Plot
ggplot(corrs) +
  geom_path(aes(x = prop_nn, y = corr, colour = img), alpha = 0.1, show.legend = F)

ggplot(corrs) +
  geom_boxplot(aes(x = prop_nn, y = corr, group = prop_nn))


```

We want the proportion of NN that gives the best correlation.

Consider using the median?

```{r}
med_corrs <- corrs %>% 
  group_by(prop_nn) %>% 
  summarise(med = median(corr)) %>% 
  ungroup()

ggplot(med_corrs) +
  geom_hline(yintercept = med_corrs %>% filter(med == max(med)) %>% pull(med), colour = "red") +
  geom_point(aes(x = prop_nn, y = med)) +
  scale_y_continuous(limits = c(0,1)) +
  labs(y = "Correlation median")
```

### Varying number of points and multiple NN

```{r}
# Sizes of set of points
n_pts <- c(15:25)

# Select images that have between 15 and 25 points (one image for each value)
img_sel <- rand_points %>% 
  count(img_name) %>% 
  filter(n %in% n_pts)

# Compute correlations
corrs <- sapply(n_pts, function(i){ # across number of points
  
  # Get image name
  img_names <- img_sel %>% filter(n == i) %>% pull(img_name)
  
  # Get points within this image
  r_points <- rand_points %>% filter(img_name %in% img_names)
  
  # Compute correlation over images
  sapply(unique(r_points$img_name), function(i_img){ # across images
  
  # Get points corresponding to given image
    r_points_img <- r_points %>% filter(img_name == i_img)
    
    # Convert to point pattern
    x2D <- ppp(
      r_points_img$x, 
      r_points_img$y, 
      window = owin(xrange = c(1, 10240), yrange = c(1, 2048))
    )
    x3D <- pp3(
      r_points_img$x, 
      r_points_img$y, 
      r_points_img$z, 
      box3(xrange = c(1, 10240), yrange = c(1, 2048), zrange = c(1, 10240))
    )
    
    # Compute distance to k NN and correlation between 2D and 3D distance
    sapply(c(2:(i-1)), function(k_nn) { # across number of NN
      nn2D <- nndist(x2D, k = k_nn)
      nn3D <- nndist(x3D, k = k_nn)
      cor(nn2D, nn3D, method = type_corr) 
    })
  })
})

# This returns a list, 1 element of the list = 1 size of set of points
# In each list,
# - cols are images
# - rows are number of considered NN

# These have different shape, we will pad with columns of NA up to the max number of image for a given number of points
img_max <- img_sel %>% count(n, name = "n_img") %>% pull(n_img) %>% max()

pad_df <- function(df, n_img_max){
  # number of dummy columns/images to generate
  n_dummy_img <- img_max - ncol(df)
  # number of NN
  n_nn <- nrow(df)
  
  df <- bind_cols(
    df,
    matrix(NA, nrow = n_nn, ncol = n_dummy_img) %>% 
      as_tibble()
  )
  # Add columns names (separately to have the right number of columns)
  df <- df %>% setNames(paste0("img_", 1:ncol(df)))
}

# Apply padding to all df
corrs <- lapply(corrs, pad_df, n_img_max = img_max)

# Bind lists into a dataframe
corrs <- do.call(rbind, corrs) %>% as_tibble()

# Columns are images
# We need to fill
# - number of points
# - number of considered NN
corrs <- corrs %>% 
  mutate(
    # List of size of set of points for df
    n_pt = unlist(sapply(n_pts, function(i){rep(i, length.out = i - 2)})),
    # List of k NN to consider within each set
    k_nn = unlist(sapply(n_pts, function(i){seq(from = 2, to = i-1)})),
    prop_nn = k_nn / n_pt,
    .before = img_1
  )

# Rearrange to vertical
corrs <- corrs %>% 
  pivot_longer(contains("img"), names_to = "img", values_to = "corr")

```

Have a look at the median.

```{r}
#| warning: false
med_corrs <- corrs %>% 
  group_by(n_pt, prop_nn) %>% 
  summarise(med = median(corr, na.rm = TRUE)) %>% 
  ungroup()

best_med <- med_corrs %>% filter(med == max(med)) %>% pull(med)

ggplot(med_corrs) + 
  geom_path(aes(x = prop_nn, y = med, group = n_pt, colour = n_pt)) +
  scale_colour_viridis_c() +
  scale_y_continuous(limits = c(0,1)) +
  labs(y = "Correlation median")
```

```{r}
rand_points

```

## From 2D to 3D?

### Compute all distances in 2D

```{r}
# Loop over images and compute distances between all points within each image
dist_all_2d <- mclapply(unique(rand_points$img_name), function(name) {
  # Get points within image
  points <- rand_points %>% 
    filter(img_name == name) %>% 
    select(x, y) %>% 
    as.matrix()
  # Compute distances between points
  melt(as.matrix(dist(points)), varnames = c("p1", "p2")) %>% 
    as_tibble() %>% 
    filter(p1 != p2) %>% 
    rename(dist = value) %>% 
    mutate(img_name = name) 
}, mc.cores = n_cores)

# Combine results in a dataframe
dist_all_2d <- do.call(bind_rows, dist_all_2d) %>% 
    # Keep only one of distances compute between A and B (A to B and B to A were computed)
    mutate(pair = ifelse(p1 < p2, paste(p1, p2), paste(p2, p1))) %>% 
    distinct(img_name, pair, dist, .keep_all = TRUE) %>% 
    select(-pair)

```

### Compute all distances in 3D

```{r}
# Loop over images and compute distances between all points within each image
dist_all_3d <- mclapply(unique(rand_points$img_name), function(name) {
  # Get points within image
  points <- rand_points %>% 
    filter(img_name == name) %>% 
    select(x, y, z) %>% 
    as.matrix()
  # Compute distances between points
  melt(as.matrix(dist(points)), varnames = c("p1", "p2")) %>% 
    as_tibble() %>% 
    filter(p1 != p2) %>% 
    rename(dist = value) %>% 
    mutate(img_name = name) 
}, mc.cores = n_cores)

# Combine results in a dataframe
dist_all_3d <- do.call(bind_rows, dist_all_3d) %>% 
    # Keep only one of distances compute between A and B (A to B and B to A were computed)
    mutate(pair = ifelse(p1 < p2, paste(p1, p2), paste(p2, p1))) %>% 
    distinct(img_name, pair, dist, .keep_all = TRUE) %>% 
    select(-pair)
```

### Compare 3D and 2D

We can expect distances in 2D to be 2/3 of distances in 3D. Let’s plot this:

-   black line shows upper bound (distance in 2D cannot be larger than distance in 3D)

-   red line shows the expected 2/3 relationship

-   blue line is a linear fit with `intercept = 0`

```{r}
df <- tibble(
  dist_2D = dist_all_2d$dist,
  dist_3D = dist_all_3d$dist
  ) 
df %>% 
  slice_sample(n = 1000) %>% 
  ggplot(aes(x = dist_3D, y = dist_2D)) +
  geom_abline(slope = 1, intercept = 0) +
  geom_abline(slope = 2/3, intercept = 0, colour = "red") +
  geom_point() +
  stat_smooth(method = "lm", formula = "y ~ x + 0") 
```

Note too bad! Let’s actually fit the model to get the slope.

```{r}
mod <- lm(dist_2D ~ 0 + dist_3D, data = df)
summary(mod)
```

Indeed, close to the expected 2/3 ratio, but with a lot of (expected) heteroskedasticity.

::: {.callout-note icon="false"}
## Conclusion

**Use all distances within an image! 3D distance could be computed from 2D distance.**
:::
