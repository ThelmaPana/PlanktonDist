---
title: "Correct distances in x axis"
subtitle: "Extract solitary Collodaria, prepare Ecotaxa import, get data from Ecotaxa."
author: "Thelma Pana√Øotis"
format: 
  html:
    toc: true
    embed-resources: true
editor: visual
lightbox: true
execute:
  warning: false
  cache: true
  freeze: false
---

```{r r_set_up}
#| echo: false
#| cache: false
source("utils.R")

read_from_ecotaxa <- FALSE # whether to read solitary collodaria from Ecotaxa
```

```{python py_set_up}
#| echo: false
#| cache: false
#| eval: false

import pandas as pd
import numpy as np
import os
import tarfile
import cv2
import matplotlib.pyplot as plt

# Path to apeep processed data
apeep_path = '/home/tpanaiotis/complex/visufront/apeep_processed'

# Path to write images
vig_path = 'data/solitary_collodaria/images'
```

## Extract solitary collodaria and prepare Ecotaxa import

### Get objects of interest

```{r read}
#| cache.lazy: false

## All data, because not so many solitary collodaria
images <- read_parquet("data/00.images_clean.parquet")
plankton <- read_parquet("data/00.plankton_clean.parquet")

# Get solitary Collodaria
plankton <- plankton %>% filter(taxon == "Collodaria_solitaryblack")

# List img names
images <- images %>% filter(img_name %in% plankton$img_name)
img_names <- sort(unique(images$img_name))

```

### Prepare table for Ecotaxa

First, we need to prepare some new columns.

```{r new_cols}
plankton_eco <- plankton %>% 
  mutate(
    # Date and time as YYYYMMDD and HHMMSS
    date = datetime %>% as_date() %>% as.character() %>% str_replace_all("-", ""),
    time = datetime %>% as_hms() %>% as.character() %>% str_replace_all(":", ""),
    depth_max = depth, # for depth_max
    # path to vignette
    img_file_name = paste("images", img_name, paste0(object_id, ".png"), sep = "/"),
    transect = str_remove_all(transect, "0") # remove "0" in transect name
  ) %>% 
  select(
    # select relevant columns and rename
    object_id,
    img_file_name,
    sample_id = transect,
    process_id = img_name,
    object_lon = lon,
    object_lat = lat,
    object_date = date,
    object_time = time,
    object_depth_min = depth,
    object_depth_max = depth_max,
    object_dist = dist,
    object_x = x,
    object_y = y,
    object_height = height,
    object_width = width
  )
```

Now move to python to add header row and save as tsv.

```{python first_row}
run = False
if run:
  df = r.plankton_eco
  
  # add first row, containing data format codes; [f] for floats, [t] for text
  # initiate first_row as floats
  first_row = ['[f]'] * (df.shape[1])
  
  # list of possible columns with data format as text [t]
  as_text = [
      'img_file_name',
      'object_id',
      'sample_id',
      'process_id'
  ]
  
  # for columns in particles_props and with text format, change first row to [t]
  col_ind_text = [df.columns.get_loc(col) for col in list(set(df_ecotaxa.columns) & set(as_text  ))]
  for i in col_ind_text:
      first_row[i] = '[t]'
      
  # first_row as Dataframe row with appropriate headers
  first_row = pd.DataFrame(first_row).T
  first_row.columns = df.columns
  
  # concat first_row and dataframe
  df_ecotaxa = pd.concat([first_row, df], ignore_index = True)
  
  ## Save as tsv file
  df_ecotaxa.to_csv('data/solitary_collodaria/ecotaxa_solitary_collodaria.tsv', index = False,   sep = '\t', header = True)
```

Now that the table is ready, we have to prepare images.

### Prepare images

::: {.callout-note icon="false"}
This part is to be run at LOV only (because images are at LOV).
:::

```{python move_images}
run = False
if run:
  df['img_path'] = df['sample_id'] + '/particles/' + df['process_id']
  df = df.sort_values(by = 'img_path').reset_index(drop = True)
  
  # put this in a list
  img_path = df['img_path'].tolist()
  # drop duplicates
  img_path = list(set(img_path))
  img_path.sort()
  
  for i, p in enumerate(img_path):
      # progress flag
      if i + 1 % 1000 == 0:
          print(f'Done with {i+1} out of {len(img_path)}')
          
      # get concerned vignettes
      vig_name = df[df['img_path'] == p].object_id.tolist()
      vig_name = [v + '.png' for v in vig_name]
      
      # Open tar file
      tar = tarfile.open(name = os.path.join(apeep_path, p + '.tar'), mode='r')
      # Get names of all vignettes
      all_vigs = tar.getnames()
      all_vigs = [x for x in all_vigs if 'png' in x]
      
      # Get names of vignettes to extract
      vig_name = [p for p in all_vigs if os.path.basename(p) in vig_name]
      
      # Loop over vignettes to process
      for v in vig_name:
      
          # Read vignette as image
          vignette = tar.extractfile(v)
          vignette = vignette.read()
          vignette = cv2.imdecode(
              np.asarray(
                  bytearray(
                      vignette
                  ), dtype=np.uint8
              ), 0
          )/255
          
          # Save image as png
          os.makedirs(os.path.join(vig_path, os.path.basename(p)), exist_ok = True)
          _ = cv2.imwrite(
              filename = os.path.join(vig_path, os.path.basename(p), os.path.basename(v)), 
              img = (vignette * 255).astype(np.uint8)
          )
```

## Read from Ecotaxa

```{r read_eco}
if (read_from_ecotaxa){
  # Fields to extract
  fields <- "obj.orig_id,sam.orig_id,txo.display_name,obj.classif_qual"
  
  # Extract object from project
  o <- get_object_set(project_id = 11779, ProjectFilters = ProjectFilters(), fields = fields)
  
  # Get details
  sol_col <- o$details %>% 
    as_tibble(.name_repair = "minimal") %>% 
    setNames(str_split(fields, ",")[[1]]) %>% 
    # rename columns
    rename(
      object_id = obj.orig_id,
      transect_new = sam.orig_id, # missed "0" in front of transect number, keep it for sanity check
      group = txo.display_name,
      classif_qual = obj.classif_qual
    ) %>% 
    # keep only validated objects
    filter(classif_qual == "V")
  
  # Join with plankton table
  sol_col <- sol_col %>% 
    left_join(plankton, by = join_by(object_id)) %>% 
    filter(transect_new == str_remove_all(transect, "0")) %>%  # make sure that transects are ok
    select(transect, img_name:taxon, group) %>% 
    mutate(ratio = height/width)
  
  # Save it
  save(sol_col, file = "data/01.sol_col.Rdata")
} else {
  load("data/01.sol_col.Rdata")
}

ggplot(sol_col) + geom_density(aes(x = ratio, colour = group))
```

Most solitary collodaria are compressed in width.

```{r height_vs_width}
sol_col %>% 
  ggplot() +
  geom_point(aes(x = width, y = height, colour = group)) +
  geom_abline(slope = 1, intercept = 0)
```

The ratio between height and width is quite coherent for well segmented organisms (group `solitaryblack`), which is not the case for poorly segmented organisms (group `t001`).

```{r ratio_transects}
#| fig-column: screen-inset
#| out-width: 100%
#| fig-width: 10
#| fig-height: 7
sol_col %>% 
  filter(group == "solitaryblack") %>% 
  group_by(transect, img_name, datetime, lat, lon, dist, depth) %>% 
  summarise(ratio = mean(ratio)) %>% 
  ungroup() %>% 
  ggplot() +
  geom_point(aes(x = dist,  y = -depth, colour = ratio)) +
  scale_colour_viridis_c() +
  facet_wrap(~transect, scales = "free")
```

When looking at ratios by transects, we notice both inter and intra transect variations.

```{r ratio_dt}
#| fig-column: screen-inset
#| out-width: 100%
#| fig-width: 10
#| fig-height: 5
sol_col %>% 
  filter(group == "solitaryblack") %>% 
  group_by(transect, img_name, datetime, lat, lon, dist, depth) %>% 
  summarise(ratio = mean(ratio)) %>% 
  ungroup() %>% 
  arrange(datetime) %>% 
  ggplot() +
  geom_path(aes(x = datetime, y = ratio)) +
  facet_wrap(~transect, scales = "free_x", nrow = 4)
```

Same when we only look and time and ignore depth.

## Apply to null data

Compare error in `z` to error in `x`.

::: {.callout-important icon="false"}
All these tests are performed on one image only.
:::

Pick one image, apply various correction factors to `x` values, compute distances for all correction factors, assess how distances are wrong compared to the uncorrected image.

```{r apply}
#| include: false
# Use only one image
#rand_points <- rand_points %>% filter(img_name == "img_00001")

# Generate a random image with 30 points
set.seed(seed)
# Pick random points within image volumes
rand_points <- tibble(
    x = runif(n = 30, min = 1, max = vol$x),
    y = runif(n = 30, min = 1, max = vol$y),
    z = runif(n = 30, min = 1, max = vol$z)
  ) %>% # Add information for img name
  mutate(img_name = "img_001")
# Compute distances for this set of points
dist_all_rand <- compute_all_dist(rand_points)

# Get all correction factors from solitary collodaria
corr_factors <- sol_col %>% filter(group == "solitaryblack") %>% pull(ratio)

# Compute distances for each correction factor
dist_all <- lapply(corr_factors, function(corr){
  df <- rand_points %>% mutate(x = x * corr)
  
  res <- compute_all_dist(df) %>% mutate(corr = corr)    
  return(res)
})
dist_all <- do.call(bind_rows, dist_all)

# Join with uncorrected distance and compute error
dist_all <- dist_all %>% 
  left_join(dist_all_rand  %>% rename(truth = dist), by = join_by(p1, p2, img_name)) %>% 
  mutate(error = (dist - truth)/truth)

```

### Correlation

How do corrected distances correlate with uncorrected distances?

#### Global correlation (all correction factors)

```{r global_corr}
dist_all %>%
  ggplot(aes(x = truth, y = dist)) +
    stat_density_2d(
    geom = "raster",
    aes(fill = after_stat(density)),
    contour = FALSE
  ) + 
  scale_fill_gradient2(low = "white", high = "#4292c6") +
  geom_abline(slope = 1, intercept = 0) +
  scale_x_continuous(expand = c(0,0)) + scale_y_continuous(expand = c(0,0)) +
  labs(x = "Uncorrected distance", y = "Corrected distance")

cor(dist_all$truth, dist_all$dist, method = "pearson") 
```

#### Per factor correlation

```{r fact_corr}
dist_all %>% 
  group_by(corr) %>% 
  summarise(correl = cor(truth, dist)) %>% 
  ungroup() %>% 
  ggplot() +
  geom_point(aes(x = corr, y = correl), alpha = 0.1, size = 0.8) +
  labs(x = "Correction factor", y = "Correlation with uncorrected distances")
```

::: {.callout-note icon="false"}
Correlations are good.
:::

### Relative error

#### Global error (all correction factors)

```{r glob_error}
ggplot(dist_all) + 
  geom_histogram(aes(x = error)) +
  scale_x_continuous(labels = label_percent(), expand = c(0,0)) +
  scale_y_continuous(expand = c(0,0)) +
  labs(x = "Relative error")
summary(dist_all$error)
```

#### Per factor error

```{r fact_error}
d1 <- ggplot(dist_all) + 
  geom_histogram(aes(x = corr), fill = "white", color = "black") +
  theme_void() +
  scale_x_continuous(expand = c(0,0))

d2 <- ggplot(dist_all) + 
  geom_histogram(aes(x = error), fill = "white", color = "black") +
  theme_void() +
  scale_x_continuous(expand = c(0,0)) +
  coord_flip()

po <- dist_all  %>% 
  ggplot(aes(x = corr, y = error)) + 
  stat_density_2d(
    geom = "raster",
    aes(fill = after_stat(density)),
    contour = FALSE
  ) + 
  scale_fill_gradient2(low = "white", high = "#4292c6") +
  scale_x_continuous(expand = c(0,0)) +
  scale_y_continuous(labels = label_percent(), expand = c(0,0)) +
  labs(x = "Correction factor", y = "Relative error")

d1 + plot_spacer() + po + d2 + 
  plot_layout(
    ncol = 2, 
    nrow = 2, 
    widths = c(4, 1),
    heights = c(1, 4),
    byrow = TRUE
  )

```

The larger the correction factor, the larger the error.

::: {.callout-note icon="false"}
The median relative error is `r percent(round(median(dist_all$error), digits = 2))`.
:::

## Compare to `z` axis error

How does this error relate to the error caused by flattening the `z` axis?

Have a look at the distribution error in `z` for the same image.

```{r z_error_comp}
#| include: false
dist_all_2d <- compute_all_dist(rand_points, z_dim = FALSE)
dist_all_3d <- compute_all_dist(rand_points, z_dim = TRUE)


df_z <- dist_all_2d %>% 
  select(img_name, dist_2D = dist) %>% 
  bind_cols(dist_all_3d %>% select(dist_3D = dist)) %>% 
  mutate(error = (dist_3D - dist_2D)/dist_2D)
```

```{r z_error_plot}
ggplot(df_z) + 
  geom_histogram(aes(x = error), binwidth = 1) + 
  labs(x = "Relative error in z") + 
  scale_x_continuous(labels = label_percent(), expand = c(0,0))

ggplot(df_z) + 
  geom_histogram(aes(x = error), binwidth = 0.02) + 
  labs(x = "Relative error in z") + 
  scale_x_continuous(labels = label_percent(), expand = c(0,0), limits = c(0, 1.1))
summary(df_z$error)
```

A few points have very strong `z` axis error (far away in 3D but very close in 2D), but most errors are \< 100%.

::: {.callout-note icon="false"}
The median relative error caused by the `z` axis is `r percent(round(median(df_z$error), digits = 2))`, although it can be very high in rare cases.
:::

## Single correction factor (the median)

As the distribution of correction factors is very narrow, let‚Äôs now focus on one correction factor only: the median of correction factors. Use this correction factor as a reference to compute relative error.

```{r to_med_err}
# Use the median of ratios as correction factor
med_corr <- median(sol_col$ratio)

# new truth
med_truth <- dist_all %>% 
  filter(corr == med_corr) %>% 
  select(p1, p2, truth = dist, img_name)

# other correction factors
med_dist <- dist_all %>% 
  select(p1, p2, dist, img_name, corr)

# Join new truth and other distances
med_dist <- med_dist %>% 
  left_join(med_truth, by = join_by(p1, p2, img_name)) %>% 
  mutate(error = (dist - truth)/truth)
```

### Correlation

How do corrected distances correlate with uncorrected distances?

#### Global correlation (all correction factors)

```{r global_corr_med}
med_dist %>%
  ggplot(aes(x = truth, y = dist)) +
    stat_density_2d(
    geom = "raster",
    aes(fill = after_stat(density)),
    contour = FALSE
  ) + 
  scale_fill_gradient2(low = "white", high = "#4292c6") +
  geom_abline(slope = 1, intercept = 0) +
  scale_x_continuous(expand = c(0,0)) + scale_y_continuous(expand = c(0,0)) +
  labs(x = "Uncorrected distance", y = "Corrected distance")

cor(med_dist$truth, med_dist$dist, method = "pearson") 
```

#### Per factor correlation

```{r fact_corr_med}
med_dist %>% 
  group_by(corr) %>% 
  summarise(correl = cor(truth, dist)) %>% 
  ungroup() %>% 
  ggplot() +
  geom_point(aes(x = corr, y = correl), alpha = 0.1, size = 0.8) +
  geom_vline(xintercept = med_corr, color = "red") + 
  labs(x = "Correction factor", y = "Correlation with uncorrected distances")
```

Correlations are good.

### Relative error

#### Global error (all correction factors)

```{r glob_error_med}
ggplot(med_dist) + 
  geom_histogram(aes(x = error)) +
  scale_x_continuous(labels = label_percent(), expand = c(0,0)) +
  scale_y_continuous(expand = c(0,0)) +
  labs(x = "Relative error")
summary(med_dist$error)
summary(abs(med_dist$error))
```

#### Per factor error

```{r fact_error_med}
d1 <- ggplot(med_dist) + 
  geom_histogram(aes(x = corr), fill = "white", color = "black") +
  theme_void() +
  scale_x_continuous(expand = c(0,0))

d2 <- ggplot(med_dist) + 
  geom_histogram(aes(x = error), fill = "white", color = "black") +
  theme_void() +
  scale_x_continuous(expand = c(0,0)) +
  coord_flip()

po <- med_dist  %>% 
  ggplot(aes(x = corr, y = error)) + 
  stat_density_2d(
    geom = "raster",
    aes(fill = after_stat(density)),
    contour = FALSE
  ) + 
  geom_vline(xintercept = med_corr, color = "red") + 
  scale_fill_gradient2(low = "white", high = "#4292c6") +
  scale_x_continuous(expand = c(0,0)) +
  scale_y_continuous(labels = label_percent(), expand = c(0,0)) +
  labs(x = "Correction factor", y = "Relative error")

d1 + plot_spacer() + po + d2 + 
  plot_layout(
    ncol = 2, 
    nrow = 2, 
    widths = c(4, 1),
    heights = c(1, 4),
    byrow = TRUE
  )

```

The larger the correction factor, the larger the error.

The median relative absolute error is `r percent(round(median(abs(med_dist$error)), digits = 2))`, which is very small compared to the error caused by the lack of information in the `z` axis.

## Apply the single correction factor to plankton data

Let‚Äôs apply the correction factor to ISIIS and null data, i.e. multiply `x` axis values by `r med_corr`. We also recompute distances between null data points.

```{r apply_corr}
#| cache.lazy: false
# Load plankton
plankton <- read_parquet("data/00.plankton_clean.parquet")
# Load subsampled plankton
load("data/00.plankton_sub.Rdata")


# Apply correction factor to null and plankton data
# For plankton, we also apply the correction factor to width
#rand_points <- rand_points %>% mutate(x = x * med_corr)
plankton <- plankton %>% 
  mutate(
    x = x * med_corr,
    width = width * med_corr
  )
plankton_sub <- plankton_sub %>% 
  mutate(
    x = x * med_corr,
    width = width * med_corr
  )

# Save it
write_parquet(plankton, sink = "data/01.x_corrected_plankton_clean.parquet")
save(plankton_sub, file = "data/01.x_corrected_plankton_sub.Rdata")
save(med_corr, file = "data/01.corr_factor.Rdata")
```
